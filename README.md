# Cloud Cost Optimization using Reinforcement Learning

## ðŸŽ¯ Project Overview

This project implements a comprehensive **Reinforcement Learning (RL) framework** for optimizing cloud computing costs by intelligently selecting the most cost-effective cloud service types based on dynamic workload characteristics and pricing models. The system addresses the real-world challenge of balancing cost optimization with service reliability and performance requirements.

### Problem Statement

Cloud computing platforms offer various services (EC2 On-Demand, EC2 Spot, AWS Lambda, AWS Fargate) with different pricing models, reliability characteristics, and performance profiles. Selecting the optimal service for a given workload is complex due to:

- **Dynamic Pricing**: Spot instances have highly variable pricing (30-90% discounts)
- **Service Interruptions**: Spot instances can be terminated with minimal notice
- **Workload Variability**: Different applications have unique demand patterns
- **SLA Constraints**: Performance requirements must be maintained
- **Cost-Performance Trade-offs**: Cheaper services may have reliability risks

## ðŸš€ How the System Works

### 1. **Environment Simulation**

The system simulates a realistic cloud environment with four service types:

#### **Service Types & Characteristics**

| Service | Capacity (req/s) | Startup Time | Reliability | Pricing Model | Use Case |
|---------|------------------|--------------|-------------|---------------|----------|
| **EC2 On-Demand** | 150 | 3 minutes | 99.9% | Stable pricing (Â±5% variation) | Reliable, predictable workloads |
| **EC2 Spot** | 150 | 3 minutes | 95% | Highly variable (30-90% discount) | Cost-sensitive, fault-tolerant workloads |
| **AWS Lambda** | 100 | 0 minutes | 99.9% | Pay-per-request + compute time | Event-driven, variable workloads |
| **AWS Fargate** | 120 | 1 minute | 99.9% | Container pricing (15% premium) | Containerized applications |

#### **Dynamic Pricing Models**

```python
# On-Demand: Stable pricing with small variations
def on_demand_pricing(instances, requests, duration, time):
    price_variation = 0.95 + 0.1 * np.random.random()  # Â±5% variation
    return instances * base_price * price_variation * duration

# Spot: High discount variability
def spot_pricing(instances, requests, duration, time):
    discount = 0.3 + 0.6 * np.random.beta(2, 5)  # 30-90% discount
    return instances * base_price * (1 - discount) * duration

# Lambda: Pay-per-request model
def serverless_pricing(instances, requests, duration, time):
    request_cost = requests * 0.0000002  # $0.20 per 1M requests
    compute_cost = duration * base_price * 0.1
    return request_cost + compute_cost
```

### 2. **Workload Generation**

The system generates four distinct workload patterns to test different scenarios:

#### **Workload Types**

- **Diurnal**: Day/night patterns with predictable peaks and valleys
- **Steady**: Consistent load with small variations
- **Batch**: Large spikes followed by idle periods (data processing jobs)
- **Bursty**: Unpredictable short spikes (web traffic surges)

```python
# Example: Diurnal workload with day/night cycle
def _generate_diurnal_workload(t, rng):
    base = 200 + 150 * np.sin(2 * np.pi * t / len(t) * 2)  # Two peaks per day
    spikes = add_random_spikes(t, rng)  # 5 random spike events
    noise = rng.normal(0, 20, size=len(t))  # Gaussian noise
    return np.clip(base + spikes + noise, 0, None)
```

### 3. **Reinforcement Learning Implementation**

#### **Why Deep Q-Network (DQN)?**

DQN was chosen for several reasons:

1. **Discrete Action Space**: Service selection is naturally discrete (4 services Ã— 3 scaling actions = 12 total actions)
2. **Value-Based Learning**: DQN learns the expected long-term value of each action, perfect for cost optimization
3. **Experience Replay**: Handles the non-stationary nature of cloud pricing and workloads
4. **Target Network**: Stabilizes learning in dynamic environments
5. **Proven Performance**: DQN has shown excellent results in similar resource allocation problems

#### **State Representation**

The RL agent observes a 10-dimensional state vector:

```python
state = [
    current_demand,           # Current workload demand (req/s)
    utilization,             # Current resource utilization (0-1)
    latency,                 # Current response latency (ms)
    ec2_ondemand_instances,  # Number of EC2 On-Demand instances
    ec2_spot_instances,      # Number of EC2 Spot instances
    lambda_instances,        # Number of Lambda instances
    fargate_instances,       # Number of Fargate instances
    ec2_ondemand_price,      # Current EC2 On-Demand price
    ec2_spot_price,          # Current EC2 Spot price
    lambda_price             # Current Lambda price
]
```

#### **Action Space**

The agent selects from 12 possible actions:

```python
# Action = (service_type, scale_action)
# service_type: 0=EC2 On-Demand, 1=EC2 Spot, 2=Lambda, 3=Fargate
# scale_action: 0=scale down, 1=no change, 2=scale up

actions = [
    (0, 0), (0, 1), (0, 2),  # EC2 On-Demand: scale down, no change, scale up
    (1, 0), (1, 1), (1, 2),  # EC2 Spot: scale down, no change, scale up
    (2, 0), (2, 1), (2, 2),  # Lambda: scale down, no change, scale up
    (3, 0), (3, 1), (3, 2)   # Fargate: scale down, no change, scale up
]
```

#### **Reward Engineering**

The reward function balances cost optimization with SLA compliance:

```python
def calculate_reward(total_cost, latency, latency_target=200):
    sla_violation = 1 if latency > latency_target else 0
    penalty = sla_penalty * sla_violation  # sla_penalty = 2.0
    return -(total_cost + penalty)  # Negative because we want to minimize cost
```

**Reward Components:**
- **Primary**: Negative cost (minimize spending)
- **Penalty**: SLA violation penalty (maintain performance)
- **Balance**: Encourages cost reduction while respecting performance constraints

### 4. **Rule-Based Baselines**

To validate RL performance, the system includes five rule-based strategies:

#### **Baseline Strategies**

1. **Cost Optimized**: Always selects the cheapest available service
2. **Reliability Optimized**: Prioritizes most reliable services (avoids spot instances)
3. **Hybrid**: Uses different services based on utilization levels
4. **Workload Aware**: Adapts strategy based on demand patterns
5. **Threshold Based**: Simple threshold-based decisions

```python
# Example: Hybrid strategy
def hybrid_predict(observation):
    utilization = observation[1]
    
    if utilization > 0.8:      # High utilization â†’ use reliable service
        service_type = 0        # EC2 On-Demand
    elif utilization < 0.3:    # Low utilization â†’ use cheap service
        service_type = 1        # EC2 Spot
    else:                      # Medium utilization â†’ use serverless
        service_type = 2        # Lambda
    
    return (service_type, scale_action)
```

## ðŸ§  How RL Helps with Cost Optimization

### **1. Learning from Experience**

Unlike rule-based approaches that use fixed heuristics, RL learns optimal policies through trial and error:

- **Exploration**: Tries different service combinations to discover cost-effective strategies
- **Exploitation**: Uses learned knowledge to make optimal decisions
- **Adaptation**: Continuously adapts to changing pricing and workload patterns

### **2. Handling Complexity**

RL excels at managing the complex interactions between:

- **Multi-dimensional State**: Considers demand, utilization, latency, prices, and service availability
- **Dynamic Environment**: Adapts to changing spot prices and workload patterns
- **Long-term Planning**: Optimizes for cumulative cost over time, not just immediate decisions
- **Trade-off Balancing**: Learns to balance cost vs. reliability vs. performance

### **3. Pattern Recognition**

The neural network learns to recognize patterns in:

- **Pricing Cycles**: Identifies when spot instances are most cost-effective
- **Workload Patterns**: Adapts service selection to different demand characteristics
- **SLA Requirements**: Learns to maintain performance while minimizing costs
- **Service Interruptions**: Develops strategies to handle spot instance terminations

### **4. Continuous Improvement**

RL provides several advantages over static rule-based approaches:

- **Self-Optimization**: Continuously improves performance through experience
- **Generalization**: Learns policies that work across different workload types
- **Robustness**: Adapts to unexpected changes in pricing or demand
- **Scalability**: Can handle complex scenarios with many variables

## ðŸ“Š Implementation Details

### **Training Process**

```python
# DQN Configuration
model = DQN(
    "MlpPolicy",
    env,
    learning_rate=1e-3,           # Learning rate for neural network
    buffer_size=50000,            # Experience replay buffer size
    batch_size=64,                # Batch size for training
    gamma=0.99,                   # Discount factor for future rewards
    train_freq=4,                 # Training frequency
    target_update_interval=500,   # Target network update frequency
    exploration_fraction=0.3,     # Exploration schedule
    exploration_initial_eps=1.0,  # Initial exploration rate
    exploration_final_eps=0.05    # Final exploration rate
)

# Training loop
model.learn(total_timesteps=20000, progress_bar=True)
```

### **Evaluation Framework**

The system provides comprehensive evaluation across multiple dimensions:

#### **Key Metrics**

1. **Total Cost**: Cumulative cost over simulation period
2. **SLA Violation Rate**: Percentage of time SLA targets are violated
3. **Resource Efficiency**: Ratio of handled requests to total capacity
4. **Service Utilization**: Average instances per service type
5. **Interruption Rate**: Frequency of service interruptions

#### **Comparison Methodology**

```python
# Cross-workload evaluation
for workload_type in ["diurnal", "steady", "batch", "bursty"]:
    # Train RL model on specific workload
    rl_model = train_dqn(workload_type)
    
    # Compare with rule-based baselines
    results = compare_strategies(
        strategies={
            "rl": rl_model,
            "cost_optimized": cost_agent,
            "hybrid": hybrid_agent,
            "reliability_optimized": reliability_agent
        },
        workload_types=[workload_type],
        n_episodes=10
    )
```

## ðŸŽ¯ Why These Design Choices?

### **1. Service Selection Rationale**

**EC2 On-Demand**: Provides baseline reliability and performance
- **Why**: Most predictable service for comparison
- **Use Case**: Critical workloads requiring guaranteed availability

**EC2 Spot**: Offers significant cost savings with interruption risk
- **Why**: Represents the cost-reliability trade-off challenge
- **Use Case**: Fault-tolerant, cost-sensitive applications

**AWS Lambda**: Serverless model with pay-per-use pricing
- **Why**: Tests ability to handle variable workloads efficiently
- **Use Case**: Event-driven, sporadic workloads

**AWS Fargate**: Container service with different pricing model
- **Why**: Represents modern containerized workloads
- **Use Case**: Microservices and containerized applications

### **2. Workload Pattern Selection**

**Diurnal**: Tests adaptation to predictable patterns
- **Why**: Many real applications have daily cycles
- **Challenge**: RL must learn to anticipate demand changes

**Steady**: Tests efficiency under consistent load
- **Why**: Baseline for comparison with variable workloads
- **Challenge**: Optimizing for consistent performance

**Batch**: Tests handling of extreme load variations
- **Why**: Common in data processing and analytics
- **Challenge**: Managing large capacity changes efficiently

**Bursty**: Tests adaptation to unpredictable spikes
- **Why**: Represents web traffic and user behavior
- **Challenge**: Rapid scaling decisions under uncertainty

### **3. Reward Function Design**

The reward function was carefully designed to:

```python
reward = -(total_cost + sla_penalty * sla_violation)
```

**Rationale**:
- **Negative Cost**: Directly optimizes for cost minimization
- **SLA Penalty**: Ensures performance requirements are met
- **Balanced Weight**: sla_penalty=2.0 provides appropriate cost-performance balance
- **Simple Form**: Easy to understand and tune

### **4. State Space Design**

The 10-dimensional state captures all relevant information:

**Demand Information**: `current_demand`, `utilization`
- **Why**: Essential for capacity planning decisions

**Performance Information**: `latency`
- **Why**: Critical for SLA compliance

**Resource Information**: Service instance counts
- **Why**: Current capacity affects scaling decisions

**Pricing Information**: Current service prices
- **Why**: Cost optimization requires price awareness

## ðŸ“ˆ Expected Results

### **Performance Expectations**

Based on the implementation, we expect:

1. **Cost Reduction**: RL should achieve 10-30% cost reduction compared to naive strategies
2. **SLA Compliance**: Maintain SLA violation rates below 5%
3. **Adaptability**: RL should perform well across different workload types
4. **Service Selection**: Intelligent switching between services based on conditions

### **Key Insights**

1. **Spot Instance Usage**: RL should learn to use spot instances during low-demand periods
2. **Lambda Efficiency**: Serverless should be preferred for variable workloads
3. **On-Demand Reliability**: Reserved for high-demand, critical periods
4. **Dynamic Adaptation**: Strategy should change based on pricing and demand patterns

## ðŸ› ï¸ Project Structure

```
cloud-rl-cost/
â”œâ”€â”€ envs/                          # Environment implementation
â”‚   â”œâ”€â”€ enhanced_cloud_env.py      # Main environment with multiple services
â”‚   â”œâ”€â”€ enhanced_cloud_gym.py      # Gym wrapper for RL training
â”‚   â”œâ”€â”€ services.py                # Cloud service definitions and pricing
â”‚   â””â”€â”€ workloads.py               # Workload pattern generation
â”œâ”€â”€ rl/                           # Reinforcement learning implementation
â”‚   â”œâ”€â”€ enhanced_train_dqn.py     # DQN training and evaluation
â”‚   â””â”€â”€ evaluate.py               # Comprehensive evaluation framework
â”œâ”€â”€ baselines/                    # Rule-based baseline strategies
â”‚   â”œâ”€â”€ rule_based.py             # Implementation of baseline agents
â”‚   â””â”€â”€ compare.py                # Comparison utilities
â”œâ”€â”€ outputs/                      # Results and model outputs
â”œâ”€â”€ run_experiments.py            # Complete experiment runner
â”œâ”€â”€ demo.py                       # Simple demonstration script
â””â”€â”€ README.md                     # This file
```

## ðŸš€ Quick Start

### **1. Installation**

```bash
# Clone the repository
git clone <repository-url>
cd cloud-rl-cost

# Install dependencies
pip install numpy matplotlib gymnasium stable-baselines3 pandas
```

### **2. Run Demo**

```bash
# Quick demonstration
python demo.py
```

### **3. Run Complete Experiment**

```bash
# Full experiment with all workload types
python run_experiments.py --workload-types diurnal steady batch bursty --episodes 10 --timesteps 20000
```

### **4. Custom Training**

```python
from envs.enhanced_cloud_gym import EnhancedCloudCostGym
from stable_baselines3 import DQN

# Create environment
env = EnhancedCloudCostGym(n_steps=300, workload_type="diurnal")

# Train DQN model
model = DQN("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=20000)

# Save model
model.save("my_dqn_model")
```

## ðŸ“‹ Step-by-Step Execution Commands

### **Prerequisites Check**

```bash
# Check Python version (should be 3.8+)
python --version

# Check if pip is available
pip --version

# Navigate to project directory
cd /path/to/cloud-rl-cost
```

### **1. Environment Setup**

```bash
# Create virtual environment (recommended)
python -m venv cloud_rl_env

# Activate virtual environment
# On Windows:
cloud_rl_env\Scripts\activate
# On macOS/Linux:
source cloud_rl_env/bin/activate

# Install required packages
pip install numpy==1.24.3
pip install matplotlib==3.7.1
pip install gymnasium==0.28.1
pip install stable-baselines3==2.0.0
pip install pandas==2.0.3
pip install torch==2.0.1

# Verify installation
python -c "import numpy, matplotlib, gymnasium, stable_baselines3, pandas; print('All packages installed successfully!')"
```

### **2. Test Basic Functionality**

```bash
# Test if the project structure is correct
python -c "from envs.enhanced_cloud_gym import EnhancedCloudCostGym; print('Environment import successful!')"

# Test workload generation
python -c "from envs.workloads import generate_workload; w = generate_workload(100, seed=42); print(f'Generated workload with {len(w)} steps')"

# Test service definitions
python -c "from envs.services import get_all_services; services = get_all_services(); print(f'Available services: {list(services.keys())}')"

# Test baseline agents
python -c "from baselines.rule_based import create_agent; agent = create_agent('hybrid'); print('Baseline agent created successfully!')"
```

### **3. Run Simple Demo**

```bash
# Run the basic demonstration
python demo.py

# Expected output: Cost comparison for different strategies across workload types
```

### **4. Test Individual Components**

```bash
# Test environment with different workload types
python -c "
from envs.enhanced_cloud_gym import EnhancedCloudCostGym
import numpy as np

# Test each workload type
for workload_type in ['steady', 'diurnal', 'batch', 'bursty']:
    env = EnhancedCloudCostGym(n_steps=50, seed=42, workload_type=workload_type)
    obs, _ = env.reset()
    print(f'{workload_type}: Initial observation shape: {obs.shape}')
    
    # Take a few random actions
    for _ in range(5):
        action = (np.random.randint(0, 4), np.random.randint(0, 3))
        obs, reward, done, truncated, info = env.step(action)
        if done or truncated:
            break
    print(f'{workload_type}: Episode completed')
"

# Test rule-based agents
python -c "
from baselines.rule_based import create_agent
import numpy as np

# Test all agent types
agent_types = ['cost_optimized', 'reliability_optimized', 'hybrid', 'workload_aware', 'threshold']
for agent_type in agent_types:
    agent = create_agent(agent_type)
    # Create sample observation
    obs = np.array([250.0, 0.6, 150.0, 2.0, 1.0, 0.0, 1.0, 0.9, 0.3, 0.1])
    action = agent.predict(obs)
    print(f'{agent_type}: Action = {action}')
"
```

### **5. Train Individual Models**

```bash
# Train DQN on steady workload (quick test)
python -c "
from rl.enhanced_train_dqn import train_enhanced_dqn
model, env = train_enhanced_dqn(
    workload_type='steady',
    n_steps=100,
    total_timesteps=5000,
    seed=42
)
print('Training completed!')
"

# Train DQN on diurnal workload
python -c "
from rl.enhanced_train_dqn import train_enhanced_dqn
model, env = train_enhanced_dqn(
    workload_type='diurnal',
    n_steps=200,
    total_timesteps=10000,
    seed=42
)
print('Diurnal training completed!')
"
```

### **6. Run Complete Experiments**

```bash
# Quick experiment (5 episodes, 100 steps, 5000 timesteps)
python run_experiments.py \
    --workload-types steady diurnal \
    --episodes 5 \
    --steps 100 \
    --timesteps 5000 \
    --output-dir quick_results

# Medium experiment (10 episodes, 200 steps, 10000 timesteps)
python run_experiments.py \
    --workload-types steady diurnal batch \
    --episodes 10 \
    --steps 200 \
    --timesteps 10000 \
    --output-dir medium_results

# Full experiment (20 episodes, 300 steps, 20000 timesteps)
python run_experiments.py \
    --workload-types diurnal steady batch bursty \
    --episodes 20 \
    --steps 300 \
    --timesteps 20000 \
    --output-dir full_results

# Custom experiment with specific parameters
python run_experiments.py \
    --workload-types diurnal \
    --episodes 15 \
    --steps 500 \
    --timesteps 30000 \
    --output-dir custom_results \
    --seed 123
```

### **7. Evaluate Trained Models**

```bash
# Load and evaluate a trained model
python -c "
from stable_baselines3 import DQN
from envs.enhanced_cloud_gym import EnhancedCloudCostGym
from rl.enhanced_train_dqn import evaluate_enhanced_model

# Load trained model
model = DQN.load('outputs/dqn_enhanced_diurnal')

# Create evaluation environment
env = EnhancedCloudCostGym(n_steps=300, seed=42, workload_type='diurnal')

# Evaluate model
results = evaluate_enhanced_model(model, env, n_episodes=5)
print(f'Average reward: {sum(results[\"total_rewards\"]) / len(results[\"total_rewards\"]):.2f}')
print(f'Average cost: {sum(results[\"total_costs\"]) / len(results[\"total_costs\"]):.2f}')
"
```

### **8. Compare Strategies**

```bash
# Run comprehensive comparison
python -c "
from run_experiments import run_complete_experiment

# Run comparison experiment
results = run_complete_experiment(
    workload_types=['diurnal', 'steady'],
    n_episodes=10,
    n_steps=200,
    total_timesteps=10000,
    output_dir='comparison_results',
    seed=42
)
print('Comparison experiment completed!')
"
```

### **9. Generate Visualizations**

```bash
# Check if results were generated
ls -la outputs/

# View generated plots
# On Windows:
start outputs/*.png
# On macOS:
open outputs/*.png
# On Linux:
xdg-open outputs/*.png

# Check detailed results
cat outputs/detailed_results.json | head -20
cat outputs/summary_statistics.json
```

### **10. Advanced Usage**

```bash
# Train multiple models for different workloads
for workload in steady diurnal batch bursty; do
    echo "Training model for $workload workload..."
    python -c "
from rl.enhanced_train_dqn import train_enhanced_dqn
model, env = train_enhanced_dqn(
    workload_type='$workload',
    n_steps=300,
    total_timesteps=15000,
    seed=42
)
print('$workload model trained successfully!')
"
done

# Run evaluation on all trained models
python -c "
import os
from stable_baselines3 import DQN
from envs.enhanced_cloud_gym import EnhancedCloudCostGym
from rl.enhanced_train_dqn import evaluate_enhanced_model

workloads = ['steady', 'diurnal', 'batch', 'bursty']
for workload in workloads:
    model_path = f'outputs/dqn_enhanced_{workload}'
    if os.path.exists(model_path + '.zip'):
        model = DQN.load(model_path)
        env = EnhancedCloudCostGym(n_steps=200, seed=42, workload_type=workload)
        results = evaluate_enhanced_model(model, env, n_episodes=3)
        avg_cost = sum(results['total_costs']) / len(results['total_costs'])
        print(f'{workload}: Average cost = ${avg_cost:.2f}')
"
"
```

### **11. Troubleshooting Commands**

```bash
# Check for common issues
python -c "
try:
    import torch
    print(f'PyTorch version: {torch.__version__}')
    print(f'CUDA available: {torch.cuda.is_available()}')
except ImportError:
    print('PyTorch not installed')

try:
    import stable_baselines3
    print(f'Stable Baselines3 version: {stable_baselines3.__version__}')
except ImportError:
    print('Stable Baselines3 not installed')
"

# Test environment creation
python -c "
try:
    from envs.enhanced_cloud_gym import EnhancedCloudCostGym
    env = EnhancedCloudCostGym(n_steps=10, seed=42)
    obs, _ = env.reset()
    print(f'Environment created successfully. Observation shape: {obs.shape}')
except Exception as e:
    print(f'Environment creation failed: {e}')
"

# Check file permissions
ls -la envs/ rl/ baselines/

# Verify Python path
python -c "import sys; print('Python path:'); [print(p) for p in sys.path]"
```

### **12. Clean Up**

```bash
# Remove generated files (optional)
rm -rf outputs/*.png outputs/*.json outputs/*.zip

# Deactivate virtual environment
deactivate

# Remove virtual environment (optional)
rm -rf cloud_rl_env
```

## ðŸŽ¯ Command Summary

### **Essential Commands (Minimum to Run)**

```bash
# 1. Setup
pip install numpy matplotlib gymnasium stable-baselines3 pandas

# 2. Test
python demo.py

# 3. Run experiment
python run_experiments.py --episodes 10 --timesteps 10000
```

### **Complete Workflow**

```bash
# 1. Setup environment
python -m venv cloud_rl_env
source cloud_rl_env/bin/activate  # or cloud_rl_env\Scripts\activate on Windows
pip install numpy matplotlib gymnasium stable-baselines3 pandas

# 2. Test components
python -c "from envs.enhanced_cloud_gym import EnhancedCloudCostGym; print('OK')"

# 3. Run demo
python demo.py

# 4. Run full experiment
python run_experiments.py --workload-types diurnal steady batch bursty --episodes 20 --timesteps 20000

# 5. Check results
ls outputs/
cat outputs/summary_statistics.json
```

### **Performance Tuning Commands**

```bash
# High-performance training (more episodes, more timesteps)
python run_experiments.py \
    --workload-types diurnal steady batch bursty \
    --episodes 50 \
    --steps 500 \
    --timesteps 100000 \
    --output-dir high_performance_results

# Quick testing (fewer episodes, fewer timesteps)
python run_experiments.py \
    --workload-types steady \
    --episodes 3 \
    --steps 50 \
    --timesteps 1000 \
    --output-dir quick_test_results
```

## ðŸ“Š Results and Analysis

The system generates comprehensive results including:

1. **Cost Comparison Charts**: Visual comparison of costs across strategies
2. **SLA Performance Analysis**: Violation rates and latency performance
3. **Service Usage Patterns**: How different strategies use various services
4. **Performance Trade-offs**: Cost vs. SLA violation scatter plots
5. **Time Series Analysis**: Demand, capacity, and cost over time

## ðŸ”¬ Technical Deep Dive

### **Neural Network Architecture**

The DQN uses a Multi-Layer Perceptron (MLP) with:
- **Input Layer**: 10 neurons (state dimensions)
- **Hidden Layers**: 2 layers with 64 neurons each
- **Output Layer**: 12 neurons (action dimensions)
- **Activation**: ReLU for hidden layers, linear for output
- **Optimizer**: Adam with learning rate 1e-3

### **Experience Replay**

```python
# Experience buffer stores (state, action, reward, next_state, done) tuples
buffer_size = 50000
batch_size = 64

# Training samples random batches from experience buffer
for batch in experience_buffer.sample(batch_size):
    states, actions, rewards, next_states, dones = batch
    # Update Q-network using TD learning
```

### **Target Network**

```python
# Target network is updated every 500 steps
target_update_interval = 500

# Stabilizes learning by using fixed target values
target_q_values = target_network(next_states).max(1)[0]
expected_q_values = rewards + gamma * target_q_values * (1 - dones)
```

## ðŸŽ¯ Conclusion

This project demonstrates how **Reinforcement Learning can effectively optimize cloud costs** by:

1. **Learning Optimal Policies**: DQN learns to select the best service for each situation
2. **Handling Complexity**: Manages multiple services, pricing models, and workload patterns
3. **Balancing Trade-offs**: Optimizes cost while maintaining performance requirements
4. **Adapting Dynamically**: Responds to changing conditions in real-time

The system provides a **comprehensive framework** for cloud cost optimization that can be extended to include additional services, pricing models, and optimization objectives. The combination of realistic simulation, multiple baseline strategies, and thorough evaluation makes it a valuable tool for understanding and improving cloud resource management.

---

**Key Takeaway**: Reinforcement Learning enables intelligent, adaptive cloud cost optimization that outperforms traditional rule-based approaches by learning optimal policies through experience and adapting to dynamic conditions.
